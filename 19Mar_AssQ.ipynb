{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2cc247-d6b8-41d9-a1c8-f116036a2748",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its  application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930163fa-6c67-4845-92e3-9db81237def7",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896acf8-d6ff-4592-8d0c-56a57470b129",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data procesing technique that scales the value of a feature to a fixed range usually between 0 and 1.it is useful when the features have different ranges but we want to normalize them into the same scale. the formula for min-max scaling is:\n",
    "\n",
    "X_scaled  = (X - X_min)/X_min - X_max\n",
    "\n",
    "where X is the original value, where as X_min and X_max are the minimum and maximum values of the feture.\n",
    "\n",
    "suppose we have dataset with two features: age and income, age ranges from 0 to 100, and income ranges from 0 to 100,000. we want to scale both features between 0 and 100, we can do that using min-max scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b3637b-cfa5-4330-93e0-5efbe599168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.33333333 0.16      ]\n",
      " [0.66666667 0.56      ]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "X= np.array([[20,22000],[30,30000],\n",
    "             [40,50000],[50,72000]])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2bd53-8387-4c6b-8588-e4f42fbb1a57",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?  Provide an example to illustrate its application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607af30d-edf4-4dce-b1a8-4b232466131a",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7fc97e-734d-4029-b8f8-07e43da2c8e8",
   "metadata": {},
   "source": [
    "Unit Vector technique also known as Normalization is a data processing technique where we can scale the value of the feature between the fixed range between 0 and 1.It is similar to the min-max scaling but instead of scaling between the minimum and maximum, we can scale the values between the length of the feature vector.\n",
    "\n",
    "for example:\n",
    " suppose we have a dataset with two features: age and income. Age ranges from 0 to 100, while income ranges from 0 to 100,000. We want to scale both features to the feature vector using Unit Vector technique. We can apply Unit Vector technique as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dfa41b-a092-4d45-9d99-86e5b397bf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.99999938e-04 9.99999875e-01]\n",
      " [6.66666519e-04 9.99999778e-01]\n",
      " [4.99999938e-04 9.99999875e-01]\n",
      " [3.99996800e-03 9.99992000e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "X= np.array([[25,50000],[30,45000],[35,70000],[40,10000]])\n",
    "\n",
    "normalizer=Normalizer()\n",
    "X_scaled = normalizer.fit_transform(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f7aee-2223-4c3a-b652-1cf354d11fc9",
   "metadata": {},
   "source": [
    "The difference between Min-Max scaling and Unit Vector technique is that Min-Max scaling scales the values of a feature to a fixed range between minimum and maximum, while Unit Vector technique scales the values of a feature to the length of the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaccb0-76c3-4f9f-b40e-9e79b328c534",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an  example to illustrate its application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33c7e4-a33f-4580-aab5-472fee56d467",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51854c3-e5c7-4245-ad77-334abd4b8ea0",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a statistical technique used to reduce the dimensionality of a dataset while retaining as much of the original information as possible. It works by identifying the principal components of the data, which are linear combinations of the original features that capture the most variation in the data. PCA is useful when working with high-dimensional data, where it can be difficult to visualize and analyze the data.\n",
    "\n",
    "PCA can be used for dimensionality reduction by projecting the data onto a lower-dimensional space while retaining as much of the original information as possible. The number of principal components to keep is determined by how much variance they explain in the data. The more principal components we keep, the more information we retain, but at the cost of increased complexity.\n",
    "\n",
    "Here’s an example of how PCA can be used for dimensionality reduction:\n",
    "\n",
    "Suppose we have a dataset with 3 features: height, weight, and shoe size. We want to reduce the dimensionality of this dataset to 2 dimensions using PCA. We can apply PCA as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ff2ca6-9d17-45e6-9b75-c72d85afdeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.37909678 -1.01626273]\n",
      " [-3.69945892  1.19612575]\n",
      " [ 8.52445238 -0.69433632]\n",
      " [ 3.55410332  0.51447329]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[4,15,6],[5,20,7],[10,28,15],[7,25,12]])\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "x_pca = pca.fit_transform(x)\n",
    "print(x_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2980f-40bc-4c69-a01a-5547c8159e1d",
   "metadata": {},
   "source": [
    "As you can see, we have reduced the dimensionality of our dataset from 3 to 2 while retaining most of the original information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e87db4-a02f-47b9-bed3-ac18057cc501",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature  Extraction? Provide an example to illustrate this concept. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473848c-b01c-4839-b878-9ebb39fd4d5c",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021d1a4-d2c1-43d8-8c37-bbb161a87513",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a type of feature extraction technique that can be used to reduce the dimensionality of a dataset while retaining as much of the original information as possible. PCA works by identifying the principal components of the data, which are linear combinations of the original features that capture the most variation in the data.\n",
    "\n",
    "PCA can be used for feature extraction by projecting the data onto a lower-dimensional space while retaining as much of the original information as possible. The number of principal components to keep is determined by how much variance they explain in the data. The more principal components we keep, the more information we retain, but at the cost of increased complexity.\n",
    "\n",
    "PCA is often used as a preprocessing step for machine learning algorithms to reduce the dimensionality of high-dimensional datasets. By reducing the number of features, we can improve the performance of machine learning algorithms and reduce overfitting.\n",
    "\n",
    "Here’s an example of how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with 4 features: age, income, height, and weight. We want to reduce the dimensionality of this dataset to 2 dimensions using PCA. We can apply PCA as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0602fb31-635d-4038-86a4-6b7544311ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.50000056e+04  1.40898261e-15]\n",
      " [-5.00000187e+03 -5.99080316e-16]\n",
      " [ 5.00000187e+03  5.99080316e-16]\n",
      " [ 1.50000056e+04  1.00959573e-15]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[25, 50000, 170, 70], [30, 60000, 175, 75], [35, 70000, 180, 80], [40, 80000, 185, 85]])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(X)\n",
    "print(x_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c80cd6-3cdd-4b68-a0b8-affc1c64e21a",
   "metadata": {},
   "source": [
    "As you can see, we have reduced the dimensionality of our dataset from 4 to 2 while retaining most of the original information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6998eeea-9962-4138-9298-efeba7bb5462",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset  contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to  preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e312a4-b644-45fb-b298-037a663dfd30",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14f3ac-0d3d-4a6e-9f4f-12be194d3371",
   "metadata": {},
   "source": [
    "In the context of a food delivery service recommendation system, Min-Max scaling can be used to preprocess the data by scaling the values of each feature to a fixed range between 0 and 1. This is useful when the features have different ranges, and we want to normalize them to the same scale.\n",
    "\n",
    "For example, suppose we have a dataset with three features: price, rating, and delivery time. Price ranges from 0 to 100, rating ranges from 0 to 5, and delivery time ranges from 0 to 60 minutes. We want to scale all three features to a range between 0 and 1. We can apply Min-Max scaling as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "411b62ea-a24a-4c1b-ba2e-c9da8a36aea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42857143 0.         0.        ]\n",
      " [0.78571429 1.         1.        ]\n",
      " [0.         1.         0.5       ]\n",
      " [1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "X= np.array([[50,4,20],[75,5,40],[20,5,30],[90,5,40]])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_scaled = scaler.fit_transform(X)\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313a8d4-9753-4b2f-94d7-5c495081285d",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many  features, such as company financial data and market trends. Explain how you would use PCA to reduce the  dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6542fe-fbb8-49eb-b548-1669535cde1d",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252d4cb-7d8b-4c4f-a04c-167555ff6ffd",
   "metadata": {},
   "source": [
    "In the context of a project to predict stock prices, Principal Component Analysis (PCA) can be used to reduce the dimensionality of the dataset by identifying the principal components of the data, which are linear combinations of the original features that capture the most variation in the data. By reducing the number of features, we can improve the performance of machine learning algorithms and reduce overfitting.\n",
    "\n",
    "PCA works by projecting the data onto a lower-dimensional space while retaining as much of the original information as possible. The number of principal components to keep is determined by how much variance they explain in the data. The more principal components we keep, the more information we retain, but at the cost of increased complexity.\n",
    "\n",
    "Here’s an example of how PCA can be used for dimensionality reduction in a stock price prediction project:\n",
    "\n",
    "Suppose we have a dataset with 10 features: company financial data and market trends. We want to reduce the dimensionality of this dataset to 5 dimensions using PCA. We can apply PCA as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6446503-9d02-4f01-98d4-3e6ad4af0a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0786305   0.00681502 -0.1717246  -0.11972289  0.18456256]\n",
      " [ 0.22444989  0.25005096 -0.28678874 -0.25612216  0.3146473 ]\n",
      " [-0.39760426  0.16452989  0.01853192  0.35214628  0.12261278]\n",
      " ...\n",
      " [ 0.13856791 -0.06306234  0.06547262 -0.24364704  0.08825853]\n",
      " [ 0.16809991 -0.00943217 -0.37695786 -0.2179353   0.18228137]\n",
      " [ 0.01935798  0.54284706  0.11707303  0.05689358  0.30103826]]\n"
     ]
    }
   ],
   "source": [
    "X= np.random.rand(1000,10)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "x_pca= pca.fit_transform(X)\n",
    "\n",
    "print(x_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfc515-92ec-4b9e-a605-7a6bcc468a1b",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the  values to a range of -1 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ec651-2f2e-4e7c-a2fa-154306be8e2f",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de85b35-ba93-41f5-a70e-734bb8e7f9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import numpy as np\n",
    "\n",
    "X=np.array([[1,5,10,15,20]])\n",
    "\n",
    "new_min=-1\n",
    "new_max= 1\n",
    "\n",
    "old_min=np.min(X)\n",
    "old_max=np.max(X)\n",
    "\n",
    "X_scaled = ((X - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b69a9a-fa06-4485-ae5f-9639c56ec5d8",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform  Feature Extraction using PCA. How many principal components would you choose to retain, and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c2c68-c9eb-43dd-a1bf-21fb6cc7b15e",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd260788-96af-4efc-b9b9-4ae67d3a9525",
   "metadata": {},
   "source": [
    "In the context of a dataset containing the following features: [height, weight, age, gender, blood pressure], we can apply PCA as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5a83e49-2888-4b0a-961d-76c3cb38f4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22418512 0.20503565 0.19727443 0.1879835  0.1855213 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "X= np.random.rand(1000,5)\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Calculate the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349d636-ff0c-4f3b-8a8f-f28a59795d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
