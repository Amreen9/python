{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81242a3-b46a-47a4-bdbb-aafeb3a9f777",
   "metadata": {},
   "source": [
    "# Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0ff3e-366c-4c9a-98b1-1df0400f8630",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "### Answer:\n",
    "Lasso Regression is a regulization technique used in machine learning to estimate the relationships between variables and make predictions. It is a type of linear regression that adds a penalty term to the cost function, which tends to zero out some features’ coefficients, making it useful for feature selection.\n",
    "\n",
    "Lasso Regression differs from other regression techniques such as Ridge Regression in the way it adds the penalty term to the cost function. Lasso Regression uses soft thresholding, which shrinks the coefficients of some features to zero, while Ridge Regression shrinks the coefficients towards zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d77ae3-67bf-400e-9cda-fbf2fc35793a",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "### Answer:\n",
    "The main advantage of using Lasso Regression in feature selection is that it can help identify the most important features in a dataset, making the model more interpretable. By adding a penalty term to the cost function, Lasso Regression reduces the risk of overfitting on the training data, which is particularly useful when dealing with high-dimensional datasets.\n",
    "\n",
    "Lasso Regression is also capable of reducing the number of features in a model by shrinking the coefficients of some features to zero, effectively removing them from the model. This makes the model less complex and easier to interpret, thereby minimizing the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7a4ef-feeb-4971-8bbb-42ed2d013c9d",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "### Answer:\n",
    "The coefficients of a Lasso Regression model can be interpreted in a similar way to those of a linear regression model. The coefficients represent the change in the response variable for a unit change in the predictor variable, while holding all other predictor variables constant.\n",
    "\n",
    "However, since Lasso Regression adds a penalty term to the cost function, some of the coefficients may be shrunk to zero, effectively removing the corresponding predictor variables from the model. This makes the model more interpretable and reduces the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f835a-8667-4f3a-89dc-adae23a97d56",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "### Answer:\n",
    "In Lasso Regression, the tuning parameter is called alpha. It controls the strength of the penalty term added to the cost function, which in turn affects the model’s performance.\n",
    "\n",
    "The value of alpha can be adjusted to achieve the desired level of regularization. A higher value of alpha results in stronger regularization, which tends to zero out more coefficients and reduce the model’s complexity. On the other hand, a lower value of alpha results in weaker regularization, which allows more coefficients to remain non-zero and increases the model’s complexity.\n",
    "\n",
    "The optimal value of alpha depends on the specific dataset and the problem at hand. One common approach to selecting the optimal value of alpha is to use cross-validation, which involves splitting the data into training and validation sets and testing the model’s performance on each set for different values of alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e640dc-d6aa-495e-8ca5-03402374b3cd",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "### Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb844da1-40bc-452e-82b8-62eeba21770d",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can be used for non-linear regression problems. However, it is not a straightforward process. One way to use Lasso Regression for non-linear regression is to transform the input variables to a higher dimension using a non-linear function and then apply Lasso Regression to the transformed variables. Another way is to use a kernel function to map the input variables to a higher dimension and then apply Lasso Regression to the transformed variables. However, the choice of kernel function is critical and requires domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1103f6a-ed7a-4c3e-b050-490b01d47ff5",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "### Answer:\n",
    "The main difference between Ridge Regression and Lasso Regression is the way they shrink the coefficients. Ridge Regression shrinks the coefficients towards zero by adding a penalty term proportional to the square of the magnitude of the coefficients. This penalty term is also known as the L2 regularization term. On the other hand, Lasso Regression shrinks the coefficients towards zero by adding a penalty term proportional to the absolute value of the magnitude of the coefficients. This penalty term is also known as the L1 regularization term.\n",
    "\n",
    "Another difference between Ridge Regression and Lasso Regression is that Lasso Regression tends to make coefficients to absolute zero as compared to Ridge Regression which never sets the value of the coefficient to absolute zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246a4bf-4327-4483-9319-a3a12c64d718",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "### Answer:\n",
    "Lasso Regression struggles with multicollinearity in the input features. Multicollinearity occurs when two or more input features are highly correlated with each other. In such cases, Lasso Regression tends to select one of the correlated features and sets the coefficients of the other correlated features to zero. This can lead to a loss of information and a decrease in the predictive power of the model.\n",
    "\n",
    "However, there are some methods that can be used to handle multicollinearity in Lasso Regression. One such method is to use a combination of Lasso Regression and Principal Component Analysis (PCA). PCA is a technique that can be used to reduce the dimensionality of the input features by transforming them into a new set of uncorrelated features called principal components. These principal components can then be used as input features for Lasso Regression. This can help to reduce the impact of multicollinearity on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b5ba3-19f2-4ca6-b59a-b1303b369ad4",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "### Answer:\n",
    "Choosing the optimal value of the regularization parameter (lambda) in Lasso Regression is an important step in building a good model. The optimal value of lambda is the one that minimizes the prediction error of the model while also keeping the model simple.\n",
    "\n",
    "There are several methods for choosing the optimal value of lambda in Lasso Regression. One such method is cross-validation. In this method, the data is divided into several subsets, and the model is trained on each subset while using the remaining subsets for validation. The value of lambda that gives the best performance on the validation set is chosen as the optimal value of lambda.\n",
    "\n",
    "Another method is to use information criteria such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) to choose the optimal value of lambda. These criteria balance the goodness of fit of the model with the complexity of the model.\n",
    "\n",
    "A third method is to use grid search. In this method, a range of values for lambda is specified, and the model is trained on each value of lambda. The value of lambda that gives the best performance on the test set is chosen as the optimal value of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ae5a3-6e0d-4855-8037-4bd9fc7c2559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
