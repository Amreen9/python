{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e49421-aae7-4587-ab78-564681dda083",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "### Answer:\n",
    "Gradient Boosting Regression is a powerful machine learning algorithm used for both regression and classification tasks. It belongs to the ensemble learning family and combines multiple weak learners (typically shallow decision trees) to create a robust predictive model. Here’s how it works:\n",
    "\n",
    "1. Ensemble Approach:\n",
    "    - Gradient boosting builds an ensemble of weak models sequentially.\n",
    "    - Each new model corrects the errors made by the previous ones.\n",
    "    \n",
    "    1. Key Concepts:\n",
    "    - Weak Learners: These are simple models (e.g., decision stumps) that perform slightly better than random guessing.\n",
    "\n",
    "2. Boosting: The algorithm focuses on challenging examples by adjusting sample weights during training.\n",
    "    1. Training Process:\n",
    "    - Initialize with equal weights for all samples.\n",
    "    - Train the first weak model.\n",
    "    - Update sample weights based on misclassifications.\n",
    "    - Train the next model using the updated weights.\n",
    "        Repeat until a predefined number of models (estimators) are built.\n",
    "\n",
    "3. Aggregation:\n",
    "    - Combine predictions from all models, weighted by their performance.\n",
    "    - The final prediction is an ensemble of these weighted contributions.\n",
    "4. Parameters:\n",
    "    - Parameters like n_estimators, max_depth, min_samples_split, and learning_rate control the model’s behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdc5b5-66a3-4356-ac02-058d22a10e3a",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1144bd-dce5-41a4-94bc-433e17c13742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65d82da-b8b0-42a0-8744-ef903da8da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes=datasets.load_diabetes()\n",
    "x,y=diabetes.data,diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799a9dae-aebe-49bd-a387-8f877eb7877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4a577e-30a9-43c8-943a-b262a5fb355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'n_estimators':500,\n",
    "    'max_depth':4,\n",
    "    'min_samples_split':5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9d804e-cee1-4d4d-8dda-ae0799354180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 3833.8047\n"
     ]
    }
   ],
   "source": [
    "reg=ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(x_train,y_train)\n",
    "\n",
    "mse=mean_squared_error(y_test,reg.predict(x_test))\n",
    "\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a0d873-ea1e-47e8-b560-b09c0bb7486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the r2 score is :0.3595885429591954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(f'the r2 score is :{r2_score(y_test,reg.predict(x_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e23c2c-7a2f-42b8-8593-6da69c1a8e80",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674f2a0-8cb5-44fd-848f-e53c07bc3fc7",
   "metadata": {},
   "source": [
    "## Creating Synthetic Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdea5139-191b-4ec6-9747-439fdc236d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a9f414-978e-448b-ba04-ee03994f1af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=make_regression(n_samples=1000,n_features=5,n_informative=3,noise=10,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962a12b-658e-4b3f-a1f3-6ba2aa77508c",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a11cd1-c5da-42af-a6f8-ab6e9d25d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf43840-46f7-4420-9d64-ab0b0ab0a35d",
   "metadata": {},
   "source": [
    "## Define paramgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b801886b-ac8f-4971-ae21-5264a8e171dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid={\n",
    "    \n",
    "    'n_estimators':[100,200,300],\n",
    "    'max_depth':[3,5,7],\n",
    "    'learning_rate':[0.01,0.1,0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54204e70-1013-41f0-b7f6-2dbcc642aef3",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f2cdeb1-35c5-4a30-af4f-890a5b154ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paramgrid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gbm\u001b[38;5;241m=\u001b[39mGradientBoostingRegressor()\n\u001b[0;32m----> 2\u001b[0m grid_scv\u001b[38;5;241m=\u001b[39mGridSearchCV(gbm,param_grid\u001b[38;5;241m=\u001b[39m\u001b[43mparamgrid\u001b[49m,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paramgrid' is not defined"
     ]
    }
   ],
   "source": [
    "gbm=GradientBoostingRegressor()\n",
    "grid_scv=GridSearchCV(gbm,param_grid=paramgrid,n_jobs=-1,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea3874-c261-4d41-953e-46e4593fae29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ffbb1-c284-4e2c-abcf-fab9250c2ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ca891-e2bd-4762-a846-c69dfa9599b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a68f6c-6770-42ac-9ac9-7e286a0ae23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3b353-531a-41af-aefb-f29bc5ed23f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d27f027b-159b-4687-ad45-bc6878502db0",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "### answer:\n",
    "In **Gradient Boosting**, a **weak learner** refers to a simple, low-complexity model that performs slightly better than random guessing. These weak models are often shallow decision trees (also known as decision stumps). Here are some key points about weak learners:\n",
    "\n",
    "1. **Characteristics of Weak Learners**:\n",
    "   - **Shallow Depth**: Weak learners have limited depth (few splits) to prevent overfitting.\n",
    "   - **High Bias, Low Variance**: They exhibit high bias (systematic error) but low variance (stable predictions).\n",
    "   - **Limited Expressiveness**: Weak models capture simple patterns in the data.\n",
    "\n",
    "2. **Role in Gradient Boosting**:\n",
    "   - Gradient Boosting sequentially adds weak learners to the ensemble.\n",
    "   - Each new model corrects the errors made by the previous ones.\n",
    "   - The combination of these weak models results in a strong, accurate ensemble.\n",
    "\n",
    "3. **Boosting Mechanism**:\n",
    "   - Weak learners focus on challenging examples by adjusting sample weights during training.\n",
    "   - The algorithm adapts to misclassified samples, emphasizing their importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c6b57-47ce-47a2-83e7-e8cb87252b45",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "### Answer:\n",
    "The intuition behind the **Gradient Boosting** algorithm lies in repetitively leveraging patterns in residuals to strengthen a model with weak predictions and make it better. Here's how it works:\n",
    "\n",
    "1. **Ensemble Approach**:\n",
    "   - Gradient Boosting starts by fitting an initial model (e.g., a tree or linear regression) to the data.\n",
    "   - Then, it builds a second model that focuses on accurately predicting cases where the first model performs poorly.\n",
    "   - The combination of these two models is expected to be better than either model alone.\n",
    "   - This process is repeated multiple times, with each successive model correcting for the shortcomings of the combined ensemble.\n",
    "\n",
    "2. **Minimizing Prediction Error**:\n",
    "   - The key idea is that the best possible next model, when combined with previous models, minimizes the overall prediction error.\n",
    "   - To achieve this, the target outcomes for the next model are set to minimize the error.\n",
    "\n",
    "**Example (Regression)**:\n",
    "Suppose we want to predict a candidate's salary based on experience and degree:\n",
    "   - We start with a base model (average of actual outputs).\n",
    "   - Calculate the pseudo residual (actual salary - predicted salary).\n",
    "   - Create a decision tree using experience and degree as inputs and the residual as the output.\n",
    "   - Combine the base model and the new tree's predictions.\n",
    "   - Adjust predictions using a learning rate (to prevent overfitting).\n",
    "   - Repeat the process, gradually reducing residuals and approaching the actual value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642dc400-2944-408f-abe2-b481aceedf89",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "### answer:\n",
    "**Gradient Boosting** constructs an ensemble of weak learners (typically simple decision trees) in an iterative manner. Here's how it builds the ensemble:\n",
    "\n",
    "1. **Weak Learners**:\n",
    "   - Weak learners are models that perform only slightly better than random chance.\n",
    "   - They focus on simple patterns and have limited complexity (e.g., shallow decision trees).\n",
    "\n",
    "2. **Boosting Process**:\n",
    "   - Start with a single weak learner (often a decision tree).\n",
    "   - Identify examples that the weak learner misclassified.\n",
    "   - Build another weak learner that specifically targets the areas where the first one failed.\n",
    "   - Repeat this process, creating a sequence of weak learners.\n",
    "   - Each new learner corrects the errors made by the previous ones.\n",
    "\n",
    "3. **Tuning to Weak Points**:\n",
    "   - The more often an example is misclassified, the more likely the next weak learner will correctly classify it.\n",
    "   - All weak learners work together to form a single strong learner.\n",
    "\n",
    "4. **Comparison with Random Forests**:\n",
    "   - Similarities:\n",
    "     - Both use tree models and aggregate predictions.\n",
    "     - Both have high inter-group diversity.\n",
    "   - Differences:\n",
    "     - Boosting trains trees iteratively, whereas random forests train them independently².\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588bc06c-a9ff-4319-9d11-00602204afc3",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "### answer:\n",
    "Certainly! Let's explore the steps involved in constructing the mathematical intuition behind the **Gradient Boosting** algorithm:\n",
    "\n",
    "1. **Boosting Overview**:\n",
    "   - Boosting is an ensemble technique that combines multiple weak models (e.g., decision trees) to create a strong model.\n",
    "   - The intuition is to iteratively correct the errors made by previous models.\n",
    "\n",
    "2. **Step-by-Step Process**:\n",
    "   - Here's how Gradient Boosting works mathematically:\n",
    "\n",
    "   a. **Build a Base Model (M1)**:\n",
    "      - Train an initial model (e.g., a decision tree) on the training dataset.\n",
    "      - Assume equal weights for all observations.\n",
    "\n",
    "   b. **Compute Pseudo Residuals**:\n",
    "      - Calculate the residuals (errors) between actual and predicted values using M1.\n",
    "      - These residuals represent the areas where M1 performed poorly.\n",
    "\n",
    "   c. **Build a New Model (M2)**:\n",
    "      - Update observation weights based on misclassifications by M1.\n",
    "      - Select only the misclassified observations for M2.\n",
    "      - M2 focuses on correcting M1's errors.\n",
    "\n",
    "   d. **Repeat for More Models (M3, M4, ...)**:\n",
    "      - Continue the process, building additional models (M3, M4, ...).\n",
    "      - Each new model corrects the errors of the previous ones.\n",
    "\n",
    "   e. **Combine Predictions**:\n",
    "      - Combine the predictions from all models (weighted by their performance).\n",
    "      - The final ensemble prediction is a combination of M1, M2, M3, ...\n",
    "\n",
    "   f. **Prediction for New Data**:\n",
    "      - When new data arrives, pass it through all models.\n",
    "      - The class with the highest vote becomes the final prediction.\n",
    "\n",
    "3. **Minimizing Errors**:\n",
    "   - Gradient Boosting minimizes the overall prediction error by iteratively adjusting the models.\n",
    "   - Learning rates control the contribution of each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0a31f-0be1-4f63-b6a7-c28f3eba621b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
