{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426e3b62-1bc2-4981-a4b3-f84709ee5658",
   "metadata": {},
   "source": [
    "# Regression-1\n",
    "## Assignment Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e316820-77bc-494d-9efa-508d3dc4ab97",
   "metadata": {},
   "source": [
    "---\n",
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "---\n",
    "## Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407a7a2-014c-44ab-bf93-4459c4514078",
   "metadata": {},
   "source": [
    "**Simple linear regression** is a statistical method that examines the relationship between two variables: an independent variable (X) and a dependent variable (Y). It assumes that there is a linear relationship between the two variables, i.e., a straight line can be used to model the relationship. For example, a simple linear regression can be used to model the relationship between the number of hours studied and the score obtained in an exam.\n",
    "\n",
    "**Multiple linear regression** is a statistical method that examines the relationship between a dependent variable (Y) and two or more independent variables (X1, X2, X3, …). It assumes that there is a linear relationship between the dependent variable and each of the independent variables. For example, multiple linear regression can be used to model the relationship between the price of a house (dependent variable) and its size, location, and age (independent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05550e8-086c-48d7-afa5-2da3c8a81ba6",
   "metadata": {},
   "source": [
    "---\n",
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "---\n",
    "## Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b446c-e73a-4f09-96e0-18a0ea9f1b2c",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method that examines the relationship between a dependent variable and one or more independent variables. The following are the assumptions of linear regression:\n",
    "\n",
    "1. **Linear relationship**: There should be a linear relationship between the dependent variable and the independent variable(s). This can be checked by creating a scatter plot of the dependent variable against each independent variable. If the points on the scatter plot form a straight line, then there is a linear relationship between the variables.\n",
    "\n",
    "2. **Independence**: The observations should be independent of each other. This means that the value of one observation should not be influenced by the value of another observation. This can be checked by examining the data collection method and ensuring that there is no correlation between the observations.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the errors should be constant across all levels of the independent variable(s). This can be checked by creating a scatter plot of the residuals against the predicted values. If the points on the scatter plot are randomly scattered around the horizontal axis, then the assumption of homoscedasticity is met.\n",
    "\n",
    "4. **Normality**: The residuals should be normally distributed. This can be checked by creating a histogram of the residuals and examining whether it follows a normal distribution. Alternatively, a normal probability plot can be created to check for normality.\n",
    "\n",
    "5. **No multicollinearity**: The independent variables should not be highly correlated with each other. This can be checked by calculating the correlation coefficient between each pair of independent variables. If the correlation coefficient is close to 1 or -1, then there is high multicollinearity.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can create a scatter plot of the dependent variable against each independent variable to check for linearity. You can examine the data collection method to ensure that the observations are independent of each other. You can create a scatter plot of the residuals against the predicted values to check for homoscedasticity. You can create a histogram of the residuals to check for normality. Finally, you can calculate the correlation coefficient between each pair of independent variables to check for multicollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e0b11-1ff8-4ebd-a34a-c84e8bdac1b0",
   "metadata": {},
   "source": [
    "---\n",
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "---\n",
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc54ad7-16d9-4938-8384-9878966e667b",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope represents the change in the dependent variable for a one-unit increase in the independent variable. The intercept represents the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "For example, let’s say you want to predict the price of a house based on its size. You collect data on the size and price of 50 houses and fit a linear regression model to the data. The model takes the following form:\n",
    "\n",
    "Price = 50,000 + 100(Size)\n",
    "\n",
    "The intercept of 50,000 represents the predicted price of a house when its size is zero. This doesn’t make sense in the context of the problem because a house can’t have a size of zero. However, the slope of 100 represents the predicted increase in price for a one-unit increase in size. So, if you have two houses that differ in size by one unit, the house with the larger size is predicted to be worth $100 more than the smaller house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5d8ac-1df9-42e9-a36c-3a3c240b6dc7",
   "metadata": {},
   "source": [
    "---\n",
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "---\n",
    "## Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ceb314-6463-4f5d-8e33-a6a789f05cff",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used in machine learning to minimize the cost function by iteratively adjusting the parameters in the direction of the negative gradient, aiming to find the optimal set of parameters.\n",
    "\n",
    "The basic idea behind Gradient Descent is to iteratively adjust the parameters of a model in the direction of steepest descent of the cost function. The cost function is a measure of how well the model fits the data. By minimizing the cost function, we can find the optimal set of parameters that best fit the data.\n",
    "\n",
    "The algorithm works by computing the gradient of the cost function with respect to each parameter and then updating the parameters in the opposite direction of the gradient. The size of the update is controlled by the learning rate, which determines how quickly the algorithm converges to the optimal set of parameters.\n",
    "\n",
    "Gradient Descent is used in many machine learning algorithms, including Linear Regression, Logistic Regression, Artificial Neural Networks, and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b4ddb3-5ade-423b-9194-d166ebef4b8d",
   "metadata": {},
   "source": [
    "---\n",
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "---\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b8349-0a33-4e73-be07-23834a8bc4d8",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression** is a statistical method that examines the relationship between a dependent variable and two or more independent variables. It assumes that there is a linear relationship between the dependent variable and each of the independent variables. The model can be represented as:\n",
    "\n",
    "Y = b0 + b1X1 + b2X2 + … + bnXn\n",
    "\n",
    "where Y is the dependent variable, X1, X2, …, Xn are the independent variables, and b0, b1, b2, …, bn are the coefficients of the model.\n",
    "\n",
    "The main difference between Simple Linear Regression and Multiple Linear Regression is the number of independent variables. Simple Linear Regression has only one independent variable, whereas Multiple Linear Regression has two or more independent variables. For example, Simple Linear Regression can be used to model the relationship between the number of hours studied and the score obtained in an exam, whereas Multiple Linear Regression can be used to model the relationship between the price of a house and its size, location, and age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb470c71-b0d2-4328-b79d-5348fea39e0d",
   "metadata": {},
   "source": [
    "---\n",
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "---\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff51cd0-b492-4604-90a4-53bc8f6b73cd",
   "metadata": {},
   "source": [
    "Multicollinearity is a phenomenon that occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. This can lead to problems with the interpretation of the model and can make the results less trustworthy.\n",
    "\n",
    "To detect multicollinearity in a dataset, you can use the following methods:\n",
    "\n",
    "1. Correlation matrix: A correlation matrix can be used to identify the correlation between each pair of independent variables. If two or more independent variables have a high correlation coefficient, then there may be multicollinearity in the dataset.\n",
    "\n",
    "2. Variance Inflation Factor (VIF): VIF is a measure of the degree of multicollinearity in a dataset. It measures how much the variance of the estimated regression coefficient is increased due to multicollinearity in the dataset. A VIF value of 1 indicates no multicollinearity, while a value greater than 1 indicates the presence of multicollinearity.\n",
    "\n",
    "To address the issue of multicollinearity, you can use the following methods:\n",
    "\n",
    "1. Remove one of the correlated variables: If two or more independent variables are highly correlated with each other, you can remove one of the variables from the model.\n",
    "\n",
    "2. Combine the correlated variables: You can combine the correlated variables into a single composite variable using techniques such as Principal Component Analysis (PCA).\n",
    "\n",
    "3. Use regularization techniques: Regularization techniques such as Ridge Regression and Lasso Regression can be used to reduce the impact of multicollinearity on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61def71a-4eeb-423a-b8b6-66c15b41e62f",
   "metadata": {},
   "source": [
    "---\n",
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "---\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772768b-2eb2-4aef-8566-c531bf3c47a6",
   "metadata": {},
   "source": [
    "Polynomial Regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables when the relationship is not linear. It involves fitting a polynomial function to the data points to obtain a curve that represents the relationship between the variables.\n",
    "\n",
    "The equation for a polynomial regression model can be written as:\n",
    "\n",
    "Y = a + b1X + b2X^2 + … + bnx^n\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, and n is the degree of the polynomial. The goal of polynomial regression is to find the best fit curve that represents the relationship between the variables. This curve is obtained by minimizing the sum of the squared residuals between the predicted values and the actual values.\n",
    "\n",
    "The main difference between Linear Regression and Polynomial Regression is the degree of the polynomial. Linear Regression assumes a linear relationship between the dependent variable and the independent variable, whereas Polynomial Regression can model non-linear relationships between the variables. For example, if we want to model the relationship between the price of a house and its size, Linear Regression would assume a straight-line relationship between the two variables, whereas Polynomial Regression could model a curved relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0274d-e404-4041-89c2-e4ee6b670212",
   "metadata": {},
   "source": [
    "---\n",
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "---\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690b50a-43a8-4098-9d2f-bd6f143be879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92448db6-23df-4070-ae85-284829f2d482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
